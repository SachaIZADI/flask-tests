{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutions by examples\n",
    "\n",
    "### Acknowledgement\n",
    "\n",
    "Code from Sections 1 to 3 taken from [fastai: lesson 0](http://course.fast.ai/lessons/lesson0.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math,sys,os,numpy as np\n",
    "from numpy.linalg import norm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import models,transforms,datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download MNIST data on disk and convert it to pytorch compatible formating.\n",
    "\n",
    "```torchvision.datasets``` features support (download, formatting) for a collection of popular datasets. The list of available datasets in ```torchvision``` can be found [here](http://pytorch.org/docs/master/torchvision/datasets.html).\n",
    "\n",
    "Note that the download is performed only once. The function will always check first if the data is already on disk.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: /Users/sachaizadi/Documents/Data_science_for_Business/Year2/3_DeepLearning/data\n",
       "    Transforms (if any): None\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to be modified!\n",
    "root_dir = '/Users/sachaizadi/Documents/Data_science_for_Business/Year2/3_DeepLearning/data'\n",
    "torchvision.datasets.MNIST(root=root_dir,download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST datasets consists of small images of hand-written digits. The images are grayscale and have size 28 x 28. There are 60,000 training images and 10,000 testing images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = torchvision.datasets.MNIST(root=root_dir, train=True, download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and initialize a data loader for the MNIST data already downloaded on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MNIST_dataset = torch.utils.data.DataLoader(train_set, batch_size=1, shuffle=True, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the current notebook, since we are doing no training, we can format data as _numpy ndarrays_ which are easier to plot in matplotlib. The same operations can be easily performed on _pytorch Tensors_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = train_set.train_data.numpy().astype(np.float32)/255\n",
    "b = train_set.train_labels.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) (60000,)\n"
     ]
    }
   ],
   "source": [
    "print(a.shape,b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data as compressed ndarrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(os.path.join(root_dir, 'train'), images=a, labels=b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we define a few functions for formatting and plotting our image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.load(os.path.join(root_dir, 'train.npz'))\n",
    "images=data['images']\n",
    "labels=data['labels']\n",
    "n=len(images)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fetch all images from the _eight_ class and from the _one_ class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "eights=[images[i] for i in range(n) if labels[i]==8]\n",
    "ones=[images[i] for i in range(n) if labels[i]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5851, 6742)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eights), len(ones)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: `imsave` is deprecated!\n",
      "`imsave` is deprecated in SciPy 1.0.0, and will be removed in 1.2.0.\n",
      "Use ``imageio.imwrite`` instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import scipy.misc\n",
    "scipy.misc.imsave('./eight.jpg', eights[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We improved the accuracy while reducing the embedding size from a $28\\times 28 = 784$ vector to a $4\\times 4\\times 8 = 128$ vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. Practicals: improving classification with Convolutional Neural Net\n",
    "\n",
    "You will now build a neural net that will learn the weights of the filters.\n",
    "\n",
    "The first layer of your network will be a convolutional layer with $8$ filters of size $3\\times 3$ as we did above. This will produce a (once flatten) a vector of size $128 = 3\\times 3\\times 8$. From this vector, you need to predict if the corresponding input is a $1$ or a $8$. So you are back to a classification problem or logistic regression, i.e. from there you can apply the solution of previous homework!\n",
    "\n",
    "You need to fill the code written below to construct your CNN. You will need to look for documentation about [torch.nn](https://pytorch.org/docs/stable/nn.html) in the Pytorch doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from models import classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_class = classifier()\n",
    "use_gpu = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your code should work fine on a batch of 3 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                                   -39.0323,\n",
       "                                              0.0000],\n",
       "        [                                  -117.3601,\n",
       "                                              0.0000],\n",
       "        [-5197982905302151351510909791447810048.0000,\n",
       "                                              0.0000]],\n",
       "       grad_fn=<LogSoftmaxBackward>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_3images = train_set.train_data[0:2].type(torch.FloatTensor).resize_(3, 1, 28, 28)\n",
    "conv_class(batch_3images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following lines of code implement a data loader for the train set and the test set. No modification is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_gpu else {}\n",
    "\n",
    "l8 = np.array(0)\n",
    "eights_dataset = [[torch.from_numpy(e.astype(np.float32)).unsqueeze(0), torch.from_numpy(l8.astype(np.int64))] for e in eights]\n",
    "l1 = np.array(1)\n",
    "ones_dataset = [[torch.from_numpy(e.astype(np.float32)).unsqueeze(0), torch.from_numpy(l1.astype(np.int64))] for e in ones]\n",
    "train_dataset = eights_dataset[1000:] + ones_dataset[1000:]\n",
    "test_dataset = eights_dataset[:1000] + ones_dataset[:1000]\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "    batch_size=bs, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "    batch_size=bs, shuffle=True, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You need now to code the training loop. Store the loss and accuracy for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,data_loader,loss_fn,optimizer,n_epochs=1):\n",
    "    \n",
    "    model.train(True)\n",
    "    \n",
    "    loss_train = np.zeros(n_epochs)\n",
    "    acc_train = np.zeros(n_epochs)\n",
    "    \n",
    "    for epoch_num in range(n_epochs):\n",
    "        running_corrects = 0.0\n",
    "        running_loss = 0.0\n",
    "        size = 0\n",
    "\n",
    "        for data in data_loader:\n",
    "            inputs, labels = data\n",
    "            bs = labels.size(0)\n",
    "            \n",
    "            if use_gpu:\n",
    "                inputs.cuda()\n",
    "                \n",
    "            #conv_class(batch_3images)\n",
    "            #optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "            #\n",
    "            y_pred = model(inputs)\n",
    "            loss = loss_fn(y_pred, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss\n",
    "            \n",
    "            \n",
    "            _, predicted = torch.max(y_pred.data, 1)\n",
    "            running_corrects += (predicted == labels).sum()\n",
    "            size += bs\n",
    "            \n",
    "        epoch_loss = running_loss / size\n",
    "        epoch_acc = running_corrects.item() / size\n",
    "        loss_train[epoch_num] = epoch_loss\n",
    "        acc_train[epoch_num] = epoch_acc\n",
    "        print('Train - Loss: {:.4f} Acc: {:.4f}'.format(epoch_loss, epoch_acc))\n",
    "        \n",
    "    return loss_train, acc_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0110 Acc: 0.4377\n",
      "Train - Loss: 0.0106 Acc: 0.5413\n",
      "Train - Loss: 0.0103 Acc: 0.6754\n",
      "Train - Loss: 0.0099 Acc: 0.7886\n",
      "Train - Loss: 0.0095 Acc: 0.8387\n",
      "Train - Loss: 0.0091 Acc: 0.8759\n",
      "Train - Loss: 0.0087 Acc: 0.8967\n",
      "Train - Loss: 0.0083 Acc: 0.9121\n",
      "Train - Loss: 0.0079 Acc: 0.9231\n",
      "Train - Loss: 0.0075 Acc: 0.9322\n"
     ]
    }
   ],
   "source": [
    "conv_class = classifier()\n",
    "# choose the appropriate loss\n",
    "loss_fn = nn.NLLLoss()\n",
    "learning_rate = 1e-3\n",
    "# your SGD optimizer\n",
    "optimizer_cl = torch.optim.SGD(conv_class.parameters(), lr=learning_rate)\n",
    "# and train for 10 epochs\n",
    "l_t, a_t = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's learn for 10 more epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0070 Acc: 0.9377\n",
      "Train - Loss: 0.0066 Acc: 0.9424\n",
      "Train - Loss: 0.0062 Acc: 0.9458\n",
      "Train - Loss: 0.0059 Acc: 0.9496\n",
      "Train - Loss: 0.0055 Acc: 0.9517\n",
      "Train - Loss: 0.0052 Acc: 0.9548\n",
      "Train - Loss: 0.0049 Acc: 0.9570\n",
      "Train - Loss: 0.0046 Acc: 0.9586\n",
      "Train - Loss: 0.0044 Acc: 0.9588\n",
      "Train - Loss: 0.0041 Acc: 0.9610\n"
     ]
    }
   ],
   "source": [
    "l_t1, a_t1 = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our network seems to learn but we now need to check its accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,data_loader):\n",
    "    model.train(False)\n",
    "\n",
    "    running_corrects = 0.0\n",
    "    running_loss = 0.0\n",
    "    size = 0\n",
    "\n",
    "    for data in data_loader:\n",
    "        inputs, labels = data    \n",
    "        bs = labels.size(0)\n",
    "        \n",
    "        if use_gpu:\n",
    "            inputs.cuda()\n",
    "            \n",
    "        y_pred = model(inputs)\n",
    "        loss = loss_fn(y_pred, labels)    \n",
    "        running_loss += loss\n",
    "        _, predicted = torch.max(y_pred.data, 1)\n",
    "        running_corrects += (predicted == labels).sum()\n",
    "        size += bs\n",
    "\n",
    "    print('Test - Loss: {:.4f} Acc: {:.4f}'.format(running_loss / size, running_corrects.item() / size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test - Loss: 0.0042 Acc: 0.9630\n"
     ]
    }
   ],
   "source": [
    "test(conv_class,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the optimizer to Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train - Loss: 0.0105 Acc: 0.7390\n",
      "Train - Loss: 0.0092 Acc: 0.9761\n",
      "Train - Loss: 0.0079 Acc: 0.9757\n",
      "Train - Loss: 0.0066 Acc: 0.9744\n",
      "Train - Loss: 0.0054 Acc: 0.9735\n",
      "Train - Loss: 0.0044 Acc: 0.9740\n",
      "Train - Loss: 0.0037 Acc: 0.9745\n",
      "Train - Loss: 0.0030 Acc: 0.9765\n",
      "Train - Loss: 0.0026 Acc: 0.9781\n",
      "Train - Loss: 0.0022 Acc: 0.9799\n",
      "\n",
      "\n",
      "Train - Loss: 0.0020 Acc: 0.9806\n",
      "Train - Loss: 0.0017 Acc: 0.9817\n",
      "Train - Loss: 0.0016 Acc: 0.9823\n",
      "Train - Loss: 0.0014 Acc: 0.9830\n",
      "Train - Loss: 0.0013 Acc: 0.9834\n",
      "Train - Loss: 0.0012 Acc: 0.9842\n",
      "Train - Loss: 0.0011 Acc: 0.9842\n",
      "Train - Loss: 0.0010 Acc: 0.9856\n",
      "Train - Loss: 0.0010 Acc: 0.9866\n",
      "Train - Loss: 0.0009 Acc: 0.9866\n",
      "\n",
      "\n",
      "Test - Loss: 0.0010 Acc: 0.9860\n"
     ]
    }
   ],
   "source": [
    "conv_class = classifier()\n",
    "# choose the appropriate loss\n",
    "loss_fn = nn.NLLLoss()\n",
    "learning_rate = 1e-3\n",
    "# your SGD optimizer\n",
    "optimizer_cl = torch.optim.Adam(params=conv_class.parameters(), lr=0.0001)\n",
    "\n",
    "# and train for 10 epochs\n",
    "l_t, a_t = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "l_t1, a_t1 = train(conv_class,train_loader,loss_fn,optimizer_cl,n_epochs = 10)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "test(conv_class,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many parameters did your network learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 3, 3])\n",
      "torch.Size([8])\n",
      "torch.Size([2, 128])\n",
      "torch.Size([2])\n"
     ]
    }
   ],
   "source": [
    "#sum(1 for i in conv_class.parameters())\n",
    "\n",
    "for param in conv_class.parameters():\n",
    "    print(param.size())\n",
    "    #print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Saving the full model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(conv_class, './conv_class.pk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classifier(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (fc): Linear(in_features=128, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.load('/Users/sachaizadi/Desktop/conv_class.pk')\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.04313726,\n",
       "        0.79607844, 0.8980392 , 0.1254902 , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.10196079, 0.18431373, 0.18431373, 0.11764706, 0.37254903,\n",
       "        0.99607843, 0.84313726, 0.05098039, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.1764706 , 0.6039216 , 0.7254902 , 0.7254902 ,\n",
       "        0.8745098 , 0.99215686, 0.99215686, 0.52156866, 0.6862745 ,\n",
       "        1.        , 0.7372549 , 0.07450981, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.43137255, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.9647059 , 0.6313726 , 0.89411765, 0.99215686, 0.99215686,\n",
       "        0.99607843, 0.36078432, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.5019608 , 0.9607843 , 0.99215686, 0.61960787, 0.5372549 ,\n",
       "        0.08235294, 0.        , 0.1882353 , 0.9137255 , 0.99215686,\n",
       "        0.9137255 , 0.03137255, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.54509807, 0.99607843, 0.8745098 , 0.09803922, 0.        ,\n",
       "        0.        , 0.14117648, 0.6666667 , 0.99607843, 0.95686275,\n",
       "        0.41568628, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.21568628, 0.83137256, 0.99215686, 0.6313726 , 0.04313726,\n",
       "        0.10196079, 0.69803923, 0.99215686, 0.9254902 , 0.44313726,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.02745098, 0.60784316, 0.99215686, 0.89411765, 0.3137255 ,\n",
       "        0.8745098 , 0.99215686, 0.99215686, 0.42745098, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.5529412 , 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99607843, 0.99215686, 0.6039216 , 0.11372549, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.43137255, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.99607843, 0.7019608 , 0.14901961, 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.01176471, 0.67058825, 0.99607843, 0.99607843, 0.99607843,\n",
       "        0.7019608 , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.67058825, 0.99215686, 0.99215686, 0.99215686, 0.99215686,\n",
       "        0.69803923, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.10196079, 0.48235294,\n",
       "        0.99607843, 0.99215686, 0.79607844, 0.6117647 , 0.99215686,\n",
       "        0.78431374, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.3647059 , 0.99215686,\n",
       "        0.99607843, 0.4745098 , 0.05098039, 0.3647059 , 0.99215686,\n",
       "        0.61960787, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.2509804 , 0.9372549 , 0.99215686,\n",
       "        0.29803923, 0.03137255, 0.1254902 , 0.85882354, 0.99215686,\n",
       "        0.49411765, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.52156866, 0.99607843, 0.7490196 ,\n",
       "        0.        , 0.01960784, 0.42352942, 0.91764706, 0.99607843,\n",
       "        0.41568628, 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.5176471 , 0.99215686, 0.74509805,\n",
       "        0.01960784, 0.33333334, 0.99215686, 0.9254902 , 0.6039216 ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.6       , 0.99215686, 0.6627451 ,\n",
       "        0.7529412 , 0.99215686, 0.99215686, 0.3019608 , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.4392157 , 0.99215686, 0.99215686,\n",
       "        0.99607843, 0.9254902 , 0.5058824 , 0.03529412, 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.06666667, 0.4627451 , 0.9529412 ,\n",
       "        0.7490196 , 0.44313726, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = c(torch.from_numpy(eights[0]).resize_(1, 1, 28, 28))\n",
    "torch.max(y_pred.data, 1)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = c(torch.from_numpy(ones[0]).resize_(1, 1, 28, 28))\n",
    "torch.max(y_pred.data, 1)[1].numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nav_menu": {},
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "widgets": {
   "state": {
    "0468b419a96749ec9b4cb1abdd4626f7": {
     "views": []
    },
    "2d3eeb645fa442fcb882ae96a9387e3d": {
     "views": []
    },
    "32cface5fd2d422480c840a0dbb1852d": {
     "views": []
    },
    "3d7fbc924d804aa1b0b751d1c4d9d42a": {
     "views": []
    },
    "60b62dbd86494ef0bc136aef4657b05f": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "879e65eadeba4a66bd0759b2918fa9f0": {
     "views": []
    },
    "8cd5af0fc89d43d4ae9b786c1f886bee": {
     "views": []
    },
    "c3a89a0403354dd19a296fd30376a143": {
     "views": []
    },
    "c997f4ebd8874aaea6ea7b699afc9a27": {
     "views": []
    },
    "ff487921e8134858a58437f0558fd42f": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
